{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "猫狗识别 "
    ]
   },
   "source": [
    "# 猫狗识别\n",
    "\n",
    "阅读本文你将学会**如何使用 PyTorch 进行图像识别**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔥本文 **GitHub** [https://github.com/kzbkzb/Python-AI](https://github.com/kzbkzb/Python-AI) 已收录\n",
    "\n",
    "- 作者：[K同学啊](https://mp.weixin.qq.com/s/NES9RhtAhbX_jsmGua28dA)\n",
    "- 来自专栏：《深度学习100例》-PyTorch版本\n",
    "- 数据链接：https://pan.baidu.com/s/1YREL1omT9YJrp9B1PBPTfQ （提取码：ionw）\n",
    "\n",
    "我的环境：\n",
    "|\n",
    "- 语言环境：Python3.8\n",
    "- 编译器：Jupyter lab\n",
    "- 深度学习环境：\n",
    "    - torch==1.10.0+cu113\n",
    "    - torchvision==0.11.1+cu113"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-13T07:27:43.719080Z",
     "start_time": "2024-11-13T07:27:43.704206Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取与预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对数据进行增强时注意是否合理哦！原本我使用了下面的代码对数据进行增强，试图解决数据不足的问题，经过测试我发现并非所有的增强操作都会产生正面影响。\n",
    "\n",
    "对此，我做过几个对比小实验，保持其他参数不变，仅改变数据增强方式，最后实验结果如下：\n",
    "- 不进行数据增强：79.2%\n",
    "- 随机旋转：80.8%\n",
    "- 随机旋转+高斯模糊模糊：83.3%\n",
    "- 随机垂直翻转：73.3%\n",
    "\n",
    "在《深度学习100例》后期的文章中我再进行更加详细的对比，这次算是先让大家了解一下。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-13T07:27:45.872871Z",
     "start_time": "2024-11-13T07:27:45.849197Z"
    }
   },
   "source": [
    "train_datadir = './1-cat-dog/train/'\n",
    "test_datadir  = './1-cat-dog/val/'\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),  # 将输入图片resize成统一尺寸\n",
    "    # transforms.RandomRotation(degrees=(-10, 10)),  #随机旋转，-10到10度之间随机选\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),  #随机水平翻转 选择一个概率概率\n",
    "    # transforms.RandomVerticalFlip(p=0.5),  #随机垂直翻转\n",
    "    # transforms.RandomPerspective(distortion_scale=0.6, p=1.0), # 随机视角\n",
    "    # transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),  #随机选择的高斯模糊模糊图像\n",
    "    transforms.ToTensor(),          # 将PIL Image或numpy.ndarray转换为tensor，并归一化到[0,1]之间\n",
    "    transforms.Normalize(           # 标准化处理-->转换为标准正太分布（高斯分布），使模型更容易收敛\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])  # 其中 mean=[0.485,0.456,0.406]与std=[0.229,0.224,0.225] 从数据集中随机抽样计算得到的。\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),  # 将输入图片resize成统一尺寸\n",
    "    transforms.ToTensor(),          # 将PIL Image或numpy.ndarray转换为tensor，并归一化到[0,1]之间\n",
    "    transforms.Normalize(           # 标准化处理-->转换为标准正太分布（高斯分布），使模型更容易收敛\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])  # 其中 mean=[0.485,0.456,0.406]与std=[0.229,0.224,0.225] 从数据集中随机抽样计算得到的。\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(train_datadir,transform=train_transforms)\n",
    "\n",
    "test_data  = datasets.ImageFolder(test_datadir,transform=test_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m train_datadir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./1-cat-dog/train/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m test_datadir  \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./1-cat-dog/val/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 4\u001B[0m train_transforms \u001B[38;5;241m=\u001B[39m \u001B[43mtransforms\u001B[49m\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      5\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize([\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m]),  \u001B[38;5;66;03m# 将输入图片resize成统一尺寸\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# transforms.RandomRotation(degrees=(-10, 10)),  #随机旋转，-10到10度之间随机选\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# transforms.RandomHorizontalFlip(p=0.5),  #随机水平翻转 选择一个概率概率\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# transforms.RandomVerticalFlip(p=0.5),  #随机垂直翻转\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# transforms.RandomPerspective(distortion_scale=0.6, p=1.0), # 随机视角\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),  #随机选择的高斯模糊模糊图像\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),          \u001B[38;5;66;03m# 将PIL Image或numpy.ndarray转换为tensor，并归一化到[0,1]之间\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize(           \u001B[38;5;66;03m# 标准化处理-->转换为标准正太分布（高斯分布），使模型更容易收敛\u001B[39;00m\n\u001B[0;32m     13\u001B[0m         mean\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.485\u001B[39m, \u001B[38;5;241m0.456\u001B[39m, \u001B[38;5;241m0.406\u001B[39m], \n\u001B[0;32m     14\u001B[0m         std\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.229\u001B[39m, \u001B[38;5;241m0.224\u001B[39m, \u001B[38;5;241m0.225\u001B[39m])  \u001B[38;5;66;03m# 其中 mean=[0.485,0.456,0.406]与std=[0.229,0.224,0.225] 从数据集中随机抽样计算得到的。\u001B[39;00m\n\u001B[0;32m     15\u001B[0m ])\n\u001B[0;32m     17\u001B[0m test_transforms \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m     18\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize([\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m]),  \u001B[38;5;66;03m# 将输入图片resize成统一尺寸\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),          \u001B[38;5;66;03m# 将PIL Image或numpy.ndarray转换为tensor，并归一化到[0,1]之间\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     22\u001B[0m         std\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.229\u001B[39m, \u001B[38;5;241m0.224\u001B[39m, \u001B[38;5;241m0.225\u001B[39m])  \u001B[38;5;66;03m# 其中 mean=[0.485,0.456,0.406]与std=[0.229,0.224,0.225] 从数据集中随机抽样计算得到的。\u001B[39;00m\n\u001B[0;32m     23\u001B[0m ])\n\u001B[0;32m     25\u001B[0m train_data \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mImageFolder(train_datadir,transform\u001B[38;5;241m=\u001B[39mtrain_transforms)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于 `transforms.Compose` 这部分更多的信息可以参考 https://pytorch-cn.readthedocs.io/zh/latest/torchvision/torchvision-transform/\n",
    "\n",
    "如果你想知道还有哪些数据增强手段，可以看看这里：https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py 对应的API，你可以在这里找到 https://pytorch.org/vision/stable/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([4, 3, 224, 224])\n",
      "Shape of y:  torch.Size([4]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_loader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "LeNet(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=44944, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 找到可以用于训练的 GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# 定义模型\n",
    "class LeNet(nn.Module):\n",
    "    # 一般在__init__中定义网络需要的操作算子，比如卷积、全连接算子等等\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d的第一个参数是输入的channel数量，第二个是输出的channel数量，第三个是kernel size\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 由于上一层有16个channel输出，每个feature map大小为5*5，所以全连接层的输入是16*5*5\n",
    "        self.fc1 = nn.Linear(16*53*53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 最终有10类，所以最后一个全连接层输出数量是10\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    # forward这个函数定义了前向传播的运算，只需要像写普通的python算数运算那样就可以了\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # 下面这步把二维特征图变为一维，这样全连接层才能处理\n",
    "        x = x.view(-1, 16*53*53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 损失函数与优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个损失函数和一个优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在单个训练循环中，模型对训练数据集进行预测（分批提供给它），并反向传播预测误差从而调整模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 计算预测误差\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.697082  [    0/  480]\n",
      "loss: 0.686452  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 50.8%, Avg loss: 0.692428 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.696046  [    0/  480]\n",
      "loss: 0.674288  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.690799 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.682432  [    0/  480]\n",
      "loss: 0.677850  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.686088 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.707287  [    0/  480]\n",
      "loss: 0.681919  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.681735 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.662526  [    0/  480]\n",
      "loss: 0.686361  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.678261 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.643308  [    0/  480]\n",
      "loss: 0.588915  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.661859 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.456625  [    0/  480]\n",
      "loss: 0.446218  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.660168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.416538  [    0/  480]\n",
      "loss: 0.779305  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.647555 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.622066  [    0/  480]\n",
      "loss: 0.547348  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.647476 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.690601  [    0/  480]\n",
      "loss: 0.458835  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.637805 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.441014  [    0/  480]\n",
      "loss: 0.798121  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.644360 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.340511  [    0/  480]\n",
      "loss: 0.479057  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.608323 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.435809  [    0/  480]\n",
      "loss: 0.755974  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.621828 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.403148  [    0/  480]\n",
      "loss: 0.312620  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.646973 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.165473  [    0/  480]\n",
      "loss: 0.518625  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.600993 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.328379  [    0/  480]\n",
      "loss: 0.196470  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.526722 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.021464  [    0/  480]\n",
      "loss: 0.422744  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.539513 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.140470  [    0/  480]\n",
      "loss: 0.335353  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.538070 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.265230  [    0/  480]\n",
      "loss: 0.180824  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.485590 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.277113  [    0/  480]\n",
      "loss: 0.548571  [  400/  480]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.498096 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
